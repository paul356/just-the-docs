#+OPTIONS: ^:nil
#+BEGIN_EXPORT html
---
layout: default
title: Paper Digest of December 2023
tags: [Reinforcement Learning, QD-Tree]
nav_order: {{ page.date }}
---
#+END_EXPORT

|-------------------------------------------------------------------------------------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+--------------+-------------------------------------------|
| Title                                                                                     | Authors                                                         | Synthesis                                                                                                                                   | Publisher    | Keywords                                  |
|-------------------------------------------------------------------------------------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+--------------+-------------------------------------------|
| Neural Packet Classification                                                              | Eric Liang, Ion Stoica                                          | This paper proposes using RL to construct a decision tree for packet classifiers, and it shows how to model formulate the MDP for constructing the decision tree. | SIGCOMM 2019 | Reinforcement Learning, Decision Tree     |
| Detect, Distill and Update: Learned DB Systems Facing Out of Distribution Data            | Meghdad Kurmanji, Peter Triantafillou                           | This paper presents a learning framework called DDUp for detecting OODs and updating learned database components. A statistical test is used to test the hypothesis that the new data obey the same distribution as the learned model. Knowledge Distillation is used to transfer knowledge from the old model to the new model if the hypothesis is rejected. | SIGMOD 2023  | Knowledge Distillation, Transfer Learning |
| From Large Language Models to Databases and Back - A discussion on research and education | Sihem Amer-Yahia, .etc                                          | A panel discussion on LLM and database research. They discussed about the potential impact of LLM towards databases, and what are advantages and limitations of incoperating LLM in databases. | DASFAA 2023  | LLM, Database, ChatGPT                    |
| *Fine-grained Partitioning for Aggressive Data Skipping*                                  | Liwen Sun, Michael J. Franklin, Sanjay Krishnan, Reynold S. Xin | This paper presents a feature based partitioning framework. A number of features are selected to calculate a feature vector for each tuple. The conjunction of all feature vectors in a block is used for data skipping. And data are partitioned to maximize the data skipping gain. The contributions are (1) a workload analyzer, which generates a set of features from a query log, (2) a partitioner, which computes a blocking scheme by solving a optimization problem, (3) a feature-based block skipping framework used in query execution. | SIGMOD 2014  | Partitioning, NP-Hard                     |
| Small Materialized Aggregates: A Light Weight Index Structure for Data Warehousing        | Guido Moerkotte                                                 | This paper shows how to speed up data warehouses with summaries called Small Materialized Aggregates. These SMAs are lightweight and easy to generate. They are similiar to views or data cubes but only compact information is generated for each data bucket. | VLDB 1998    | SMA, Data Cube, Data Warehouse            |
|-------------------------------------------------------------------------------------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+--------------+-------------------------------------------|

