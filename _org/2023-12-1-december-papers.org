#+OPTIONS: ^:nil
#+BEGIN_EXPORT html
---
layout: default
title: Paper Digest of December 2023
tags: [Reinforcement Learning, QD-Tree]
nav_order: {{ page.date }}
---
#+END_EXPORT

|--------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------+--------------+-------------------------------------------|
| Title                                                                          | Synthesis                                                                                                                                                                                                         | Authors                               | Publisher    | Keywords                                  |
|--------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------+--------------+-------------------------------------------|
| Neural Packet Classification                                                   | This paper proposes using RL to construct a decision tree for packet classifiers, and it shows how to model formulate the MDP for constructing the decision tree.                                                 | Eric Liang, Ion Stoica                | SIGCOMM 2019 | Reinforcement Learning, Decision Tree     |
| Detect, Distill and Update: Learned DB Systems Facing Out of Distribution Data | This paper presents a learning framework called DDUp for detecting OODs and updating learned database components. A statistical test is used to test the hypothesis that the new data obey the same distribution as the learned model. Knowledge Distillation is used to transfer knowledge from the old model to the new model if the hypothesis is rejected.           | Meghdad Kurmanji, Peter Triantafillou | SIGMOD 2023  | Knowledge Distillation, Transfer Learning |
|--------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------+--------------+-------------------------------------------|

