#+STARTUP: showall indent
#+STARTUP: hidestars
#+OPTIONS: ^:nil
#+BEGIN_EXPORT html
---
layout: default
title: Spark数据湖
tags: [Spark, 数据湖, lakehouse, iceberg]
nav_order: {{ page.date }}
---
#+END_EXPORT
* Spark数据湖

** 如果在spark中使用iceberg
#+BEGIN_SRC bash
  spark-shell --packages org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.1.0 \
              --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
              --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \
              --conf spark.sql.catalog.spark_catalog.type=hive \
              --conf spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog \
              --conf spark.sql.catalog.local.type=hadoop \
              --conf spark.sql.catalog.local.warehouse=$PWD \
              --conf spark.driver.extraJavaOptions=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=4747
#+END_SRC

** SparkCatalog
SparkCatalog是提供table的查找，打开，管理Table对象。另外一个Catalog实
现是SparkSessionCatalog，用于支持Spark内建的Catalog，比如Spark使用Hive
Metastore管理Table的路径、分区等信息时。Iceberg是通过向Spark注册了一个
SparkCatalog实现支持Iceberg在Table功能上的扩展。
SparkCatalog支持如下操作：
- 创建Iceberg格式的Table
- 打开一个Iceberg格式的Table，返回一个Iceberg的Table对象
- 创建View
- 对Table的一些配置进行修改
- 对Table改名
- 删除一个Table
SparkCatalog还依赖一些其他类实现其功能
- HadoopCatalog
- HadoopTableOperations
- BaseTable BaseTable通过TableOperations依赖于HadoopTableOperations
- SparkTable SparkTable是org.apache.spark.sql.connector.catalog.Table
  的实现，是BaseTable的proxy。

** spark写流程
- V2TableWriteExec in WriteToDataSourceV2Exec.scala

** TODO 支持分层的数据湖格式（HTAP）
- 基于lsm结构组织文件
  + 第一层采用tiering的方式排布
  + 第二层以上采用leveling的方式组织
  + 行是否唯一
- 共享索引结构
  + 通过MVCC支持一写多读
  + 对第一层的数据进行索引
    - 使用什么列来做索引内容
    - 索引指针是什么
  + 二层索引
    - 第一层range索引
    - 第二层文件内索引
- 读数据
  + 逐层读取数据，再做数据合并
  + 多个partition并发查询，从第一层过滤自己感兴趣的数据
- 写数据
  + 先写到第一层，再聚合起来下降到下层
  + 写类型
    - [X] insert into
    - [ ] merge into
    - [ ] insert overwrite
